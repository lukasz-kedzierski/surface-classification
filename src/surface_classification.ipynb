{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9afebad71082b978"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import defaultdict\n",
    "from PIL import Image\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models\n",
    "from torchvision.transforms import v2\n",
    "from torchinfo import summary"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bbbfa68a2c2ad9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "be243189c890a753"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "da0434b42f73b25a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Constants"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87b62f1e105bcc84"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 100\n",
    "IMAGE_DIR = '../data/train/'\n",
    "TEST_IMAGE_DIR = '../data/test_set_final_image_set/'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66b5555a4fa10241"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load and split data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5529717ff2812af0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "503fce34bed6d813"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c59200394bfc51d7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ImageQuery(Dataset):\n",
    "    def __init__(self, dataset, img_dir, augmentations=None, preprocessing=None):\n",
    "        self.image_paths = [img_dir + img for img in dataset.keys()]\n",
    "        # Tokenize text using CLIP's tokenizer\n",
    "        self.queries  = clip.tokenize(list(dataset.values()))\n",
    "        self.augmentations = augmentations\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.queries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.augmentations:\n",
    "            # Preprocess image using augmentation transforms\n",
    "            image = self.augmentations(Image.open(self.image_paths[idx]))\n",
    "        else:\n",
    "            image = Image.open(self.image_paths[idx])\n",
    "        # Preprocess image using CLIP's preprocessing function\n",
    "        image = self.preprocessing(image)\n",
    "        query = self.queries[idx]\n",
    "        return image, query\n",
    "\n",
    "train_dataloader = DataLoader(ImageQuery(train_dataset, IMAGE_DIR, augmentations=augment, preprocessing=clip_preprocess), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(ImageQuery(val_dataset, IMAGE_DIR, preprocessing=clip_preprocess), batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18f462da904175a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96daed4787b63f69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.resnet = models.resnet50(weights='ResNet50_Weights.IMAGENET1K_V2')\n",
    "        self.resnet = torch.nn.Sequential(*(list(self.resnet.children())[:-1]))\n",
    "        \n",
    "        for param in self.resnet.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Linear(1024, 768),\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.resnet(input)\n",
    "        output = self.model(x)\n",
    "        norm_layer = F.normalize(output)\n",
    "        return norm_layer\n",
    "    \n",
    "embedding = Embedding().to(device)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9fd5bb25deb67831"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimizer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fa4d99f3eb297adf"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    siamese.parameters(),\n",
    "    lr=1e-3,\n",
    "    eps=1e-6,\n",
    "    weight_decay=1e-3,\n",
    "    )\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7fb9f74376bf1fc"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Early stopping"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3457623a134ea771"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "early_stopper = EarlyStopper(patience=3, min_delta=1e-3)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99bf1d622f9db277"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e467a8774a8740fd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_batches = len(train_dataloader)\n",
    "val_batches = len(val_dataloader)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_train_loss = 0.0\n",
    "    running_val_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_dataloader, total=train_batches)\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_images, train_texts = batch\n",
    "\n",
    "        train_images = train_images.to(device)\n",
    "        train_texts = train_texts.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        logits_per_image, logits_per_text = model(train_images, train_texts)\n",
    "\n",
    "        # Compute loss\n",
    "        ground_truth = torch.arange(len(train_images), dtype=torch.long, device=device)\n",
    "        train_loss = (loss_img(logits_per_image, ground_truth) + loss_txt(logits_per_text, ground_truth)) / 2\n",
    "        running_train_loss += train_loss\n",
    "\n",
    "        # Backward pass\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        clip.model.convert_weights(model)\n",
    "\n",
    "        pbar.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Training loss: {running_train_loss / (idx + 1):.2E}\")\n",
    "    scheduler.step()\n",
    "\n",
    "    pbar_val = tqdm(val_dataloader, total=val_batches)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_val in enumerate(pbar_val):\n",
    "            val_images, val_texts = batch_val\n",
    "    \n",
    "            val_images = val_images.to(device)\n",
    "            val_texts = val_texts.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            val_logits_per_image, val_logits_per_text = model(val_images, val_texts)\n",
    "\n",
    "            # Compute loss\n",
    "            val_ground_truth = torch.arange(len(val_images), dtype=torch.long, device=device)\n",
    "            val_loss = (loss_img(val_logits_per_image, val_ground_truth) + loss_txt(val_logits_per_text, val_ground_truth)) / 2\n",
    "            running_val_loss += val_loss\n",
    "            \n",
    "            pbar_val.set_description(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Validation loss: {running_val_loss / (idx + 1):.2E}\")\n",
    "            \n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': running_train_loss,\n",
    "        'val_loss': running_val_loss,\n",
    "    }, f\"../data/clip_ft_{epoch + 1}.pt\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9acb973ef0d7300b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
